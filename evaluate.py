"""
å®ä½“æŠ½å–è¯„ä¼°è„šæœ¬
å®ç°5ä¸ªè¯„ä¼°æŒ‡æ ‡:
1. è¦†ç›–ç‡ - æ¯æ¡äº‹ä»¶å¹³å‡èƒ½æŠ½å‡ºå¤šå°‘å®ä½“
2. ç©ºæŠ½ç‡ - entities ä¸ºç©ºçš„æ¯”ä¾‹
3. ç±»å‹æ¨¡ç³Šç‡ - entity.type è¢«æ ‡ä¸º "other / ä¸ç¡®å®š" çš„æ¯”ä¾‹
4. ä¸€è‡´æ€§ - åŒä¸€æ–‡æœ¬å¤šæ¬¡æå–å¯¹æ¯”
5. ç±»å‹é¢‘ç‡åˆ†å¸ƒ - ç»Ÿè®¡å„ç±»å‹çš„é¢‘ç‡
"""

import json
import os
from collections import defaultdict, Counter
from typing import List, Dict
import glob
from entity_types import get_all_entity_types, get_entity_type_mapping


class EntityEvaluator:
    """å®ä½“æŠ½å–è¯„ä¼°å™¨"""

    def __init__(self, events_data: List[Dict]):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        Args:
            events_data: æå–çš„äº‹ä»¶æ•°æ®åˆ—è¡¨
        """
        self.events = events_data
        self.entity_types = get_all_entity_types()
        self.type_mapping = get_entity_type_mapping()

    def calculate_coverage(self) -> Dict:
        """
        æŒ‡æ ‡1: è¦†ç›–ç‡ - æ¯æ¡äº‹ä»¶å¹³å‡èƒ½æŠ½å‡ºå¤šå°‘å®ä½“
        Returns:
            {
                "total_events": æ€»äº‹ä»¶æ•°,
                "total_entities": æ€»å®ä½“æ•°,
                "avg_entities_per_event": å¹³å‡æ¯äº‹ä»¶å®ä½“æ•°,
                "max_entities": æœ€å¤šå®ä½“æ•°,
                "min_entities": æœ€å°‘å®ä½“æ•°
            }
        """
        if not self.events:
            return {
                "total_events": 0,
                "total_entities": 0,
                "avg_entities_per_event": 0,
                "max_entities": 0,
                "min_entities": 0
            }

        entity_counts = []
        total_entities = 0

        for event in self.events:
            entities = event.get("entities", [])
            count = len(entities)
            entity_counts.append(count)
            total_entities += count

        return {
            "total_events": len(self.events),
            "total_entities": total_entities,
            "avg_entities_per_event": round(total_entities / len(self.events), 2),
            "max_entities": max(entity_counts) if entity_counts else 0,
            "min_entities": min(entity_counts) if entity_counts else 0,
            "median_entities": sorted(entity_counts)[len(entity_counts) // 2] if entity_counts else 0
        }

    def calculate_empty_rate(self) -> Dict:
        """
        æŒ‡æ ‡2: ç©ºæŠ½ç‡ - entities ä¸ºç©ºçš„æ¯”ä¾‹
        Returns:
            {
                "total_events": æ€»äº‹ä»¶æ•°,
                "empty_events": ç©ºå®ä½“äº‹ä»¶æ•°,
                "empty_rate": ç©ºæŠ½ç‡ (0-1)
            }
        """
        if not self.events:
            return {
                "total_events": 0,
                "empty_events": 0,
                "empty_rate": 0
            }

        empty_count = 0
        for event in self.events:
            entities = event.get("entities", [])
            if not entities or len(entities) == 0:
                empty_count += 1

        return {
            "total_events": len(self.events),
            "empty_events": empty_count,
            "empty_rate": round(empty_count / len(self.events), 4)
        }

    def calculate_ambiguity_rate(self) -> Dict:
        """
        æŒ‡æ ‡3: ç±»å‹æ¨¡ç³Šç‡ - entity.type è¢«æ ‡ä¸º "other / ä¸ç¡®å®š / unknown" çš„æ¯”ä¾‹
        Returns:
            {
                "total_entities": æ€»å®ä½“æ•°,
                "ambiguous_entities": æ¨¡ç³Šç±»å‹å®ä½“æ•°,
                "ambiguity_rate": æ¨¡ç³Šç‡ (0-1),
                "ambiguous_samples": æ¨¡ç³Šå®ä½“ç¤ºä¾‹
            }
        """
        ambiguous_keywords = ["other", "ä¸ç¡®å®š", "unknown", "å…¶ä»–", "æœªçŸ¥"]
        total_entities = 0
        ambiguous_count = 0
        ambiguous_samples = []

        for event in self.events:
            entities = event.get("entities", [])
            for entity in entities:
                total_entities += 1
                entity_type = entity.get("type", "").lower()

                # æ£€æŸ¥æ˜¯å¦ä¸ºæ¨¡ç³Šç±»å‹
                is_ambiguous = any(kw in entity_type for kw in ambiguous_keywords)

                if is_ambiguous:
                    ambiguous_count += 1
                    if len(ambiguous_samples) < 10:  # åªæ”¶é›†å‰10ä¸ªç¤ºä¾‹
                        ambiguous_samples.append({
                            "name": entity.get("name", ""),
                            "type": entity.get("type", ""),
                            "description": entity.get("description", "")
                        })

        return {
            "total_entities": total_entities,
            "ambiguous_entities": ambiguous_count,
            "ambiguity_rate": round(ambiguous_count / total_entities, 4) if total_entities > 0 else 0,
            "ambiguous_samples": ambiguous_samples[:10]
        }

    def calculate_type_distribution(self) -> Dict:
        """
        æŒ‡æ ‡5: ç±»å‹é¢‘ç‡åˆ†å¸ƒ
        Returns:
            {
                "type_counts": {type: count},
                "type_percentages": {type: percentage},
                "top_types": å‰10ä¸ªé«˜é¢‘ç±»å‹,
                "rare_types": ä½é¢‘ç±»å‹ (å‡ºç°æ¬¡æ•° < æ€»æ•°çš„1%)
            }
        """
        type_counter = Counter()
        total_entities = 0

        for event in self.events:
            entities = event.get("entities", [])
            for entity in entities:
                entity_type = entity.get("type", "unknown")
                type_counter[entity_type] += 1
                total_entities += 1

        # è®¡ç®—ç™¾åˆ†æ¯”
        type_percentages = {}
        for type_name, count in type_counter.items():
            type_percentages[type_name] = round(count / total_entities * 100, 2) if total_entities > 0 else 0

        # æ‰¾å‡ºä½é¢‘ç±»å‹ (< 1%)
        threshold = total_entities * 0.01
        rare_types = {t: c for t, c in type_counter.items() if c < threshold}

        # æ‰¾å‡ºæœªä½¿ç”¨çš„å®šä¹‰ç±»å‹
        used_types = set(type_counter.keys())
        defined_types = set(self.entity_types)
        unused_types = defined_types - used_types

        return {
            "type_counts": dict(type_counter.most_common()),
            "type_percentages": type_percentages,
            "top_types": dict(type_counter.most_common(10)),
            "rare_types": rare_types,
            "unused_types": list(unused_types),
            "total_unique_types": len(type_counter)
        }

    def generate_report(self) -> str:
        """ç”Ÿæˆå®Œæ•´çš„è¯„ä¼°æŠ¥å‘Š"""
        coverage = self.calculate_coverage()
        empty_rate = self.calculate_empty_rate()
        ambiguity = self.calculate_ambiguity_rate()
        distribution = self.calculate_type_distribution()

        report = []
        report.append("=" * 80)
        report.append("å®ä½“æŠ½å–è¯„ä¼°æŠ¥å‘Š")
        report.append("=" * 80)

        # æŒ‡æ ‡1: è¦†ç›–ç‡
        report.append("\nã€æŒ‡æ ‡1ã€‘è¦†ç›–ç‡ - æ¯æ¡äº‹ä»¶å¹³å‡å®ä½“æ•°")
        report.append("-" * 80)
        report.append(f"æ€»äº‹ä»¶æ•°: {coverage['total_events']}")
        report.append(f"æ€»å®ä½“æ•°: {coverage['total_entities']}")
        report.append(f"å¹³å‡æ¯äº‹ä»¶å®ä½“æ•°: {coverage['avg_entities_per_event']}")
        report.append(f"ä¸­ä½æ•°: {coverage['median_entities']}")
        report.append(f"æœ€å¤§å€¼: {coverage['max_entities']}")
        report.append(f"æœ€å°å€¼: {coverage['min_entities']}")

        # æŒ‡æ ‡2: ç©ºæŠ½ç‡
        report.append("\nã€æŒ‡æ ‡2ã€‘ç©ºæŠ½ç‡")
        report.append("-" * 80)
        report.append(f"ç©ºå®ä½“äº‹ä»¶æ•°: {empty_rate['empty_events']}")
        report.append(f"ç©ºæŠ½ç‡: {empty_rate['empty_rate']:.2%}")
        if empty_rate['empty_rate'] > 0.15:
            report.append("âš ï¸  è­¦å‘Š: ç©ºæŠ½ç‡è¿‡é«˜ (>15%), å»ºè®®æ£€æŸ¥æç¤ºè¯æˆ–æ–‡æœ¬è´¨é‡")

        # æŒ‡æ ‡3: ç±»å‹æ¨¡ç³Šç‡
        report.append("\nã€æŒ‡æ ‡3ã€‘ç±»å‹æ¨¡ç³Šç‡")
        report.append("-" * 80)
        report.append(f"æ€»å®ä½“æ•°: {ambiguity['total_entities']}")
        report.append(f"æ¨¡ç³Šç±»å‹å®ä½“æ•°: {ambiguity['ambiguous_entities']}")
        report.append(f"ç±»å‹æ¨¡ç³Šç‡: {ambiguity['ambiguity_rate']:.2%}")
        if ambiguity['ambiguity_rate'] > 0.05:
            report.append("âš ï¸  è­¦å‘Š: ç±»å‹æ¨¡ç³Šç‡è¿‡é«˜ (>5%), å»ºè®®ä¼˜åŒ–å®ä½“ç±»å‹å®šä¹‰")
        if ambiguity['ambiguous_samples']:
            report.append("\næ¨¡ç³Šç±»å‹ç¤ºä¾‹:")
            for i, sample in enumerate(ambiguity['ambiguous_samples'][:5], 1):
                report.append(f"  {i}. {sample['name']} ({sample['type']}) - {sample['description']}")

        # æŒ‡æ ‡5: ç±»å‹åˆ†å¸ƒ
        report.append("\nã€æŒ‡æ ‡5ã€‘ç±»å‹é¢‘ç‡åˆ†å¸ƒ")
        report.append("-" * 80)
        report.append(f"ä½¿ç”¨çš„ç±»å‹æ€»æ•°: {distribution['total_unique_types']}")
        report.append(f"æœªä½¿ç”¨çš„å®šä¹‰ç±»å‹: {len(distribution['unused_types'])}")

        report.append("\nå‰10é«˜é¢‘ç±»å‹:")
        for type_name, count in list(distribution['top_types'].items())[:10]:
            pct = distribution['type_percentages'].get(type_name, 0)
            type_cn = self.type_mapping.get(type_name, type_name)
            report.append(f"  {type_name:20s} ({type_cn:10s}): {count:6d} æ¬¡ ({pct:5.2f}%)")

        if distribution['rare_types']:
            report.append(f"\nä½é¢‘ç±»å‹ (<1%, å…±{len(distribution['rare_types'])}ä¸ª):")
            for type_name, count in list(distribution['rare_types'].items())[:10]:
                type_cn = self.type_mapping.get(type_name, type_name)
                report.append(f"  {type_name:20s} ({type_cn:10s}): {count:6d} æ¬¡")

        if distribution['unused_types']:
            report.append(f"\næœªä½¿ç”¨çš„å®šä¹‰ç±»å‹ (å…±{len(distribution['unused_types'])}ä¸ª):")
            report.append(f"  {', '.join(distribution['unused_types'])}")
            report.append("ğŸ’¡ å»ºè®®: è€ƒè™‘åˆ é™¤æˆ–åˆå¹¶è¿™äº›ç±»å‹")

        report.append("\n" + "=" * 80)
        report.append("è¯„ä¼°å®Œæˆ")
        report.append("=" * 80)

        return "\n".join(report)


def calculate_consistency(file_path: str, num_runs: int = 3) -> Dict:
    """
    æŒ‡æ ‡4: ä¸€è‡´æ€§è¯„ä¼° - åŒä¸€æ–‡æœ¬å¤šæ¬¡æå–å¯¹æ¯”

    Args:
        file_path: æµ‹è¯•æ–‡ä»¶è·¯å¾„
        num_runs: æå–æ¬¡æ•°
    Returns:
        ä¸€è‡´æ€§ç»Ÿè®¡
    """
    try:
        from main import read_document, segment_into_slices, extract_events_from_slice
    except ImportError:
        return {
            "error": "æ— æ³•å¯¼å…¥ main.py ä¸­çš„å‡½æ•°,è·³è¿‡ä¸€è‡´æ€§æµ‹è¯•",
            "num_runs": 0,
            "total_unique_entities": 0,
            "inconsistent_entities": 0,
            "consistency_rate": 0,
            "inconsistent_samples": []
        }

    print(f"\næ­£åœ¨è¿›è¡Œä¸€è‡´æ€§æµ‹è¯• (å…±{num_runs}æ¬¡æå–)...")

    # è¯»å–æ–‡ä»¶å¹¶åˆ†ç‰‡
    content = read_document(file_path)
    slices = segment_into_slices(content)

    if not slices:
        return {"error": "æ— æ³•ç”Ÿæˆåˆ‡ç‰‡"}

    # åªæµ‹è¯•ç¬¬ä¸€ä¸ªåˆ‡ç‰‡
    test_slice = slices[0]
    all_runs = []

    for run in range(num_runs):
        print(f"  ç¬¬ {run + 1}/{num_runs} æ¬¡æå–...", end="", flush=True)
        events = extract_events_from_slice(test_slice, f"consistency_test_{run}")
        all_runs.append(events)
        print(f" æå–åˆ° {len(events)} ä¸ªäº‹ä»¶")

    # æ¯”è¾ƒä¸€è‡´æ€§
    entity_type_consistency = []

    for i, events in enumerate(all_runs):
        for event in events:
            entities = event.get("entities", [])
            for entity in entities:
                entity_type_consistency.append({
                    "run": i + 1,
                    "name": entity.get("name", ""),
                    "type": entity.get("type", "")
                })

    # ç»Ÿè®¡åŒä¸€å®ä½“çš„ç±»å‹ä¸€è‡´æ€§
    entity_types_map = defaultdict(list)
    for record in entity_type_consistency:
        entity_types_map[record["name"]].append(record["type"])

    inconsistent_entities = []
    for name, types in entity_types_map.items():
        unique_types = set(types)
        if len(unique_types) > 1:
            inconsistent_entities.append({
                "entity_name": name,
                "types": list(unique_types),
                "counts": dict(Counter(types))
            })

    total_unique_entities = len(entity_types_map)
    consistent_rate = 1 - (len(inconsistent_entities) / total_unique_entities) if total_unique_entities > 0 else 0

    return {
        "num_runs": num_runs,
        "total_unique_entities": total_unique_entities,
        "inconsistent_entities": len(inconsistent_entities),
        "consistency_rate": round(consistent_rate, 4),
        "inconsistent_samples": inconsistent_entities[:10]
    }


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    import sys

    if sys.platform == "win32":
        import codecs
        sys.stdout = codecs.getwriter("utf-8")(sys.stdout.detach())

    parser = argparse.ArgumentParser(description="å®ä½“æŠ½å–è¯„ä¼°å·¥å…·")
    parser.add_argument("--input", default="extracted_events.json", help="æå–ç»“æœJSONæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--consistency-test", help="ä¸€è‡´æ€§æµ‹è¯•çš„MDæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--consistency-runs", type=int, default=3, help="ä¸€è‡´æ€§æµ‹è¯•æ¬¡æ•°")
    parser.add_argument("--output", default="evaluation_report.txt", help="è¯„ä¼°æŠ¥å‘Šè¾“å‡ºè·¯å¾„")

    args = parser.parse_args()

    # è¯»å–æå–ç»“æœ
    if not os.path.exists(args.input):
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {args.input}")
        return

    with open(args.input, 'r', encoding='utf-8') as f:
        events = json.load(f)

    print(f"\nå·²åŠ è½½ {len(events)} ä¸ªäº‹ä»¶")

    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = EntityEvaluator(events)

    # ç”ŸæˆåŸºç¡€æŠ¥å‘Š
    report = evaluator.generate_report()
    print(report)

    # ä¸€è‡´æ€§æµ‹è¯•
    if args.consistency_test:
        if os.path.exists(args.consistency_test):
            consistency_result = calculate_consistency(
                args.consistency_test,
                num_runs=args.consistency_runs
            )

            consistency_report = [
                "\n" + "=" * 80,
                "ã€æŒ‡æ ‡4ã€‘ä¸€è‡´æ€§è¯„ä¼°",
                "=" * 80,
                f"æµ‹è¯•æ¬¡æ•°: {consistency_result['num_runs']}",
                f"å”¯ä¸€å®ä½“æ€»æ•°: {consistency_result['total_unique_entities']}",
                f"ç±»å‹ä¸ä¸€è‡´å®ä½“æ•°: {consistency_result['inconsistent_entities']}",
                f"ä¸€è‡´æ€§ç‡: {consistency_result['consistency_rate']:.2%}",
            ]

            if consistency_result['inconsistent_samples']:
                consistency_report.append("\nç±»å‹ä¸ä¸€è‡´ç¤ºä¾‹:")
                for sample in consistency_result['inconsistent_samples'][:5]:
                    consistency_report.append(f"  å®ä½“: {sample['entity_name']}")
                    consistency_report.append(f"  ä¸åŒç±»å‹: {sample['types']}")
                    consistency_report.append(f"  å‡ºç°æ¬¡æ•°: {sample['counts']}")
                    consistency_report.append("")

            consistency_report.append("=" * 80)
            consistency_text = "\n".join(consistency_report)
            print(consistency_text)
            report += "\n" + consistency_text
        else:
            print(f"ä¸€è‡´æ€§æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {args.consistency_test}")

    # ä¿å­˜æŠ¥å‘Š
    with open(args.output, 'w', encoding='utf-8') as f:
        f.write(report)

    print(f"\nè¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜åˆ°: {args.output}")


if __name__ == "__main__":
    main()
